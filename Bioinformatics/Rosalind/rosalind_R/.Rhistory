?facet_grid
ggplot(melt.crit, aes(nvars, value)) +
geom_line() +
facet_wrap(criterion~., nrow = 3, ncol = 2, scales = 'free_y')
fwd.mins
melt.crit
melt.min <- metl(fwd.min[,2:ncol(fwd.mins)], variable.name = 'criterion')
melt.min <- melt(fwd.min[,2:ncol(fwd.mins)], variable.name = 'criterion')
melt.min <- melt(fwd.mins[,2:ncol(fwd.mins)], variable.name = 'criterion')
melt.mins
melt.min
fwd.mins
fwd.mins < fwd.mins[,2:ncol(fwd.mins)]
fwd.mins < fwd.mins[,2:ncol(fwd.mins)-1]
fwd.mins <- fwd.mins[,2:ncol(fwd.mins)]
fwd.mins
melt.min <- melt(fwd.mins, variable.name = 'criterion')
melt.min
fwd.mins
install.packages('tidyr')
install.packages("tidyr")
install.packages("tidyr")
install.packages("tidyr")
install.packages('tidyr')
install.packages("tidyr")
install.packages("tidyr")
install.packages(gam)
install.packages(gam)
install.packages('gam')
install.packages('gam')
library(gam)
source("~/Library/CloudStorage/OneDrive-Personal/Documents/Programming/ML & NN/practical 4.R", echo=TRUE)
?glmnet
source("~/Library/CloudStorage/OneDrive-Personal/Documents/Programming/ML & NN/Principle Component Analysis (unfinished).R", echo=TRUE)
install.packages(glmnet)
install.packages('glmnet')
source("~/Library/CloudStorage/OneDrive-Personal/Documents/Programming/ML & NN/Principle Component Analysis (unfinished).R", echo=TRUE)
source("~/Library/CloudStorage/OneDrive-Personal/Documents/Programming/ML & NN/Principle Component Analysis (unfinished).R", echo=TRUE)
?glmnet
ridge
summary(ridge)
plot(ridge, xvar = 'lambda')
source("~/Library/CloudStorage/OneDrive-Personal/Documents/Programming/ML & NN/epi/epi practical 1.R", echo=TRUE)
#2.1
n_obs <- 1000000
w_true <- c(0,1)
z_obs <- data_generating_model(n = n_obs, w = w_true)
w_true <- as.numeric(glm(z_obs[,2]~1+z_obs[,1],family ="binomial")$coefficients)
batch_sto_gd <- function(w0,
tmax = 300,
eta = 0.5,
m = 10){
wt = w0
chain = c()
for (t in 1:tmax){
J = sample.int(n = n_obs, size = m, replace = TRUE)
if(m == 1){ #i.e online SGD
zbatch <- matrix(z_obs[J,],1,2)
} else{  #i.e batch SGD
zbatch <- z_obs[J,]
}
wt <- wt - eta*grad_risk_fun(wt, zbatch, m)
chain <- rbind(chain, wt)
}
return(chain)
}
sto_gd_etaconst <- batch_sto_gd(w0 = c(-0.3, 3))
plot(sto_gd_etaconst[,1], type = 'l') + abline(h = w_true[1], col = 'red')
#Section 3 - AdaGrad
set.seed(2023)
n_obs <- 1000000
w_true <- c(0,1)
z_obs <- data_generating_model(n = n_obs, w = w_true)
w_true <- as.numeric(glm(z_obs[,2]~1+z_obs[,1],family="binomial")$coefficients)
adagrad <- function(w0, tmax, eta, m, eps){
wt = w0
chain = c()
G = rep(0.0, times=length(w))
for (t in 1:tmax){
J = sample.int(n = n_obs, size = m, replace = TRUE)
if (m ==1){ #online AdaGrad
zbatch <- matrix(z_obs[J,],1,2)
} else{ #batch AdaGrad
zbatch <- z_obs[J,]
}
G <- G + (grad_risk_fun(wt, zbatch, m))^2
wt <- wt - eta*(1/sqrt(G + eps))*grad_risk_fun(wt, zbatch, m)
chain <- rbind(chain, wt)
}
return(chain)
}
onladagrad_vals <- adagrad(w0=c(-0.3,3), tmax=500, eta=1, m=1, eps=1e-06)
plot(onladagrad_vals[,1], type = 'line') + abline(h = w_true[1], col = 'red')
#Section 4 - Batch SGD with projection
n_obs <- 1000000
w_true <- c(0,1)
z_obs <- data_generating_model(n = n_obs, w = w_true)
w_true <- as.numeric(glm(z_obs[,2]~ 1+z_obs[,1],family="binomial")$coefficients)
boundary <- 2.0 # this is the value |w|_{2}^{2} <= boundary
# auxiliary functions to compute the projection
eval_f0 <- function( w_proj, w_now ){
return( sqrt(sum((w_proj-w_now)^2)) )
}
eval_grad_f0 <- function( w, w_now ){
return( c( 2*(w[1]-w_now[1]), 2*(w[2]-w_now[2]) ) )
}
eval_g0 <- function( w_proj, w_now) {
return( sum(w_proj^2) -(boundary)^2 )
}
eval_jac_g0 <- function( x, w_now ) {
return(   c(2*w[1],2*w[2] )  )
}
w <- c(-0.1,0.3)
out <- nloptr(x0=c(0.0,0.0),
eval_f=eval_f0,
eval_grad_f=eval_grad_f0,
eval_g_ineq = eval_g0,
eval_jac_g_ineq = eval_jac_g0,
w_now=w,
opts = list("algorithm" = "NLOPT_LD_MMA",
"xtol_rel"=1.0e-8)
)
batch_sgd_proj <- function(w0,
tmax = 1000,
eta = 0.1,
m = 1){
wt = w0
chain = c()
for (t in 1:tmax){
J = sample.int(n = n_obs, size = m, replace = TRUE)
if(m == 1){ #i.e online SGD
zbatch <- matrix(z_obs[J,],1,2)
} else{  #i.e batch SGD
zbatch <- z_obs[J,]
}
wt <- wt - eta*grad_risk_fun(wt, zbatch, m)
out <- nloptr(x0=c(0.0,0.0),
eval_f=eval_f0,
eval_grad_f=eval_grad_f0,
eval_g_ineq = eval_g0,
eval_jac_g_ineq = eval_jac_g0,
w_now=wt,
opts = list("algorithm" = "NLOPT_LD_MMA",
"xtol_rel"=1.0e-8)
)
wt <- out$solution
chain <- rbind(chain, wt)
}
return(chain)
}
plot(batch_sgd_proj(w0 = c(-0.3, 0.3), eta = 0.1, tmax = 1500)[,2],
type = 'line') + abline(h = w_true[2], col = 'red')
#section 5 - Stochastic Variance Reduced Gradient Descent
set.seed(2023)
n_obs <- 100000
w_true <- c(0,1)
z_obs <- data_generating_model(n = n_obs, w = w_true)
w_true <- as.numeric(glm(z_obs[,2]~ 1+ z_obs[,1],family = "binomial" )$coefficients)
svrgd <- function(w0, tmax = 500, eta = 0.5, kppa = 100, m = 1){
wt = w0
chain = c()
cv_w <- wt
erf_fun <- function(wt, z = z_obs, n=n_obs) {
return( empirical_risk_fun(wt, z, n) )
}
cv_grad_risk <- numDeriv::grad( erf_fun, cv_w ) #control variate
for (t in 1:tmax){
J = sample.int(n = n_obs, size = m, replace = TRUE)
if(m == 1){ #i.e online SGD
zbatch <- matrix(z_obs[J,],1,2)
} else{  #i.e batch SGD
zbatch <- z_obs[J,]
}
erf_fun <- function(wt, z = zbatch, n=m) {
return( empirical_risk_fun(wt, z, n) )
}
wt <- wt - eta*(numDeriv::grad( erf_fun,wt) - numDeriv::grad(erf_fun,cv_w) + cv_grad_risk)
chain <- rbind(chain, wt)
if ( (t %% kppa) == 0) {
cv_w <- wt
erf_fun <- function(wt, z = z_obs, n=n_obs) {
return( empirical_risk_fun(wt, z, n) )
}
cv_grad_risk <- numDeriv::grad( erf_fun, cv_w ) #control variate
}
}
return(chain)
}
plot(svrgd(w0 = c(-0.3, 3.0))[,1], type = 'line') + abline(h = w_true[1], col = 'red')
source("~/Library/CloudStorage/OneDrive-Personal/Documents/Programming/ML & NN/epi/Support Vector Machines.R", echo=TRUE)
install.packages(e1071)
install.packages('e1071')
install.packages('e1071')
source("~/Library/CloudStorage/OneDrive-Personal/Documents/Programming/ML & NN/epi/Support Vector Machines.R", echo=TRUE)
source("~/Library/CloudStorage/OneDrive-Personal/Documents/Programming/MLNN/epi/Support Vector Machines.R", echo=TRUE)
?scale
source("~/Library/CloudStorage/OneDrive-Personal/Documents/Programming/MLNN/epi/Artificial Neural Networks.R", echo=TRUE)
library('faraway')
data(ozone)
source("~/Library/CloudStorage/OneDrive-Personal/Documents/Programming/MLNN/epi/Artificial Neural Networks.R", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
?nnet
o3.nn <- nnet(O3 ~ temp + ibh + ibt, data = ozone,
size = 2, linout = TRUE)
names(o3.nn)
EF <- sum((o3.nn$fitted.values - ozone$O3)^2)
EF
source("~/Library/CloudStorage/OneDrive-Personal/Documents/Programming/MLNN/epi/nnet practice.R", echo=TRUE)
?apply
source("~/Library/CloudStorage/OneDrive-Personal/Documents/Programming/MLNN/epi/nnet practice.R", echo=TRUE)
source("~/Library/CloudStorage/OneDrive-Personal/Documents/Programming/MLNN/epi/nnet practice.R", echo=TRUE)
source("~/Library/CloudStorage/OneDrive-Personal/Documents/Programming/MLNN/epi/nnet practice.R", echo=TRUE)
rm(SSR)
oz.sd
head(ozone/oz.sd)
oz.s <- ozone/oz.sd
sd(oz.s)
sd(oz.s$O3)
ozone
head(ozone)
head(oz.s)
?scale
# Standardising Data for Training Multiple NNs
oz.s <- scale(ozone)
head(oz.s)
length(1:100)
?best.nnet
?check
?if
?if
else
?if
?then
?function
EF <- function(modl, coln){
return(sum((modl$fitted.values - model[,coln])^2))
}
EF(o3.nn, 1)
return(sum((modl$fitted.values - modl[,coln])^2))
EF <- function(modl, coln){
return(sum((modl$fitted.values - modl[,coln])^2))
}
EF(o3.nn, 1)
EF(o3.nn, 1)
EF <- function(modl, coln){
return(sum((modl$fitted.values - modl[1,])^2))
}
EF(o3.nn, 1)
naems(o3.nn)
names(o3.nn)
EF <- sum((o3.nn$fitted.values - O3)^2)
oz.s <- scale(ozone)
# standardises data so SD = 1 and mean = 0 for all columns
# this results in better performance from nnet since weights of connections
# in nn deal with same sized numbers
best.nn <- o3.nn
for(i in 1:100){  # loop to fit 100 nns and with different starting weights
# and find best
set.seed(i)
new.nn <- nnet(O3 ~ temp + ibh +ibt, data = oz.s,
size = 2, linout = TRUE)
EF.new <- sum((new.nn$fitted.values - O3)^2)
EF.best <- sum((best.nn$fitted.values - O3)^2)
if(EF.new < EF.best){
best.nn <- new.nn
}
}
# replace current best
best.nn <- new.nn
best.nn <- o3.nn
for(i in 1:100){  # loop to fit 100 nns and with different starting weights
# and find best
set.seed(i)
new.nn <- nnet(O3 ~ temp + ibh +ibt, data = oz.s,
size = 2, linout = TRUE)
EF.new <- sum((new.nn$fitted.values - O3)^2)
EF.best <- sum((best.nn$fitted.values - O3)^2)
if(EF.new < EF.best){ # if new model has better SSR than current best,
# then replace current best
best.nn <- new.nn
}
}
o3.nn$value
oz.s <- scale(ozone)
# standardises data so SD = 1 and mean = 0 for all columns
# this results in better performance from nnet since weights of connections
# in nn deal with same sized numbers
best.nn <- o3.nn
best.rss <- best.nn$value
for(i in 1:100){  # loop to fit 100 nns and with different starting weights
# and find best
set.seed(i)
# new seed for starting weights each iteration
new.nn <- nnet(O3 ~ temp + ibh +ibt, data = oz.s,
size = 2, linout = TRUE)
EF.new <- new.nn$value
if(EF.new < EF.best){ # if new model has better SSR than current best,
# then replace current best
best.nn <- new.nn
best.rss <- new.nn$value
}
}
rm(EF.best)
rm(best.rss)
best.nn <- o3.nn
EF.best <- best.nn$value
for(i in 1:100){  # loop to fit 100 nns and with different starting weights
# and find best
set.seed(i)
# new seed for starting weights each iteration
new.nn <- nnet(O3 ~ temp + ibh +ibt, data = oz.s,
size = 2, linout = TRUE)
EF.new <- new.nn$value
if(EF.new < EF.best){ # if new model has better SSR than current best,
# then replace current best
best.nn <- new.nn
EF.best <- new.nn$value
}
}
EF.best
SST.s <- sum((oz.s$O3 - mean(oz.s$O3))^2)
EF.best - SST.s
oz.s
?scale
oz.s[,-1]
head(oz.s)
oz.s$O3
oz.s[,1]
oz.s[,O3]
oz.s[,'O3']
SST.s <- sum((oz.s[,'O3'] - mean(oz.s[,'O3']))^2)
EF.best - SST.s
summary(best.nn)
?predict.nnet
?nnet.formula
# Plotting NN Predictions (for temp in range [-3,3] with ibh = ibt = 0)
x <- expand.grid(temp=seq(-3,3,0.1),ibh=0,ibt=0)
oz.pred <- predict(best.nn, newdata = x)
head(oz.pred)
oz.pred <- predict(best.nn, newdata = x, type = 'class')
oz.pred <- predict(best.nn, newdata = x)
oz.pred <- oz.s.pred*sd(O3) + mean(O3)
oz.s.pred <- predict(best.nn, newdata = x)
# the values predicted by the nn which used standardised data
oz.pred <- oz.s.pred*sd(O3) + mean(O3)
# reverting standardised data to physically meaningful data
head(oz.pred)
head(O3)
oz.pred
length(x$temp)
?ggplot
plot(x = x$temp, y = oz.pred)
attributes(oz.s)
sd(O3)
min(ozone$temp)
max(ozone$temp)
min(oz.s$temp)
min(oz.s[,'temp'])
max(oz[,tempdir()])
max(oz[,temp])
max(oz.s[,temp])
max(oz.s[,'temp'])
library('ggplot2')
?ggplot
x <- expand.grid(temp=seq(-3,3,0.1),ibh=0,ibt=0)
# x values used
oz.s.pred <- predict(best.nn, newdata = x)
# the values predicted by the nn which used standardised data
oz.pred <- oz.s.pred*sd(O3) + mean(O3)
# reverting standardised data to physically meaningful data
ggplot(data = c(oz.pred, x), aes(x$temp, oz.pred)) + geom_point()
?fortify
oz.pred
ggplot(data = x, aes(x$temp, oz.pred)) + geom_point()
x <- expand.grid(temp=seq(-3,3,0.1),ibh=0,ibt=0)
# standardised x values used
oz.s.pred <- predict(best.nn, newdata = x)
# the values predicted by the nn which used standardised data
oz.pred <- oz.s.pred*sd(O3) + mean(O3)
temps <- x*sd(ozone$temp) + mean(ozone$temp)
# reverting standardised data to physically meaningful data
ggplot(data = temps, aes(temps, oz.pred)) +
geom_point() +
geom_point(aes())
oz.s.pred <- predict(best.nn, newdata = x)
# the values predicted by the nn which used standardised data
oz.pred <- oz.s.pred*sd(O3) + mean(O3)
temps <- x*sd(ozone$temp) + mean(ozone$temp)
# reverting standardised data to physically meaningful data
ggplot(data = temps, aes(temps$temp, oz.pred)) +
geom_point() +
geom_point(aes())
x <- expand.grid(temp=seq(-3,3,0.1),ibh=0,ibt=0)
# standardised x values used
oz.s.pred <- predict(best.nn, newdata = x)
# the values predicted by the nn which used standardised data
oz.pred <- oz.s.pred*sd(O3) + mean(O3)
temps <- x*sd(ozone$temp) + mean(ozone$temp)
# reverting standardised data to physically meaningful data
ggplot(data = temps, aes(temp, oz.pred)) +
geom_point() +
geom_point(aes())
x.s <- expand.grid(temp=seq(-3,3,0.1),ibh=0,ibt=0)
# standardised x values used
oz.s.pred <- predict(best.nn, newdata = x)
# the values predicted by the nn which used standardised data
oz.pred <- oz.s.pred*sd(O3) + mean(O3)
x <- x*sd(ozone$temp) + mean(ozone$temp)
# reverting standardised data to physically meaningful data
ggplot(data = x, aes(temp, oz.pred)) +
geom_point() +
geom_point(aes())
ggplot(data = x, aes(temp, oz.pred)) +
geom_point() +
geom_point(data = ozone, aes(temp, O3), col = 'red')
?geom_point
ggplot(data = x, aes(temp, oz.pred)) +
geom_point() +
geom_point(data = ozone, aes(temp, O3), col = 'red', fill = 'white')
ggplot(data = x, aes(temp, oz.pred)) +
geom_point() +
geom_point(data = ozone, aes(temp, O3), col = 'red', shape = 'x')
# Plotting NN Predictions (for temp in range [-3,3] with ibh = ibt = 0)
xt.s <- expand.grid(temp=seq(-3,3,0.1),ibh=0,ibt=0)
# standardised x values used
oz.s.pred <- predict(best.nn, newdata = x)
# the values predicted by the nn which used standardised data
oz.pred <- oz.s.pred*sd(O3) + mean(O3)
xt <- xt*sd(ozone$temp) + mean(ozone$temp)
# reverting standardised data to physically meaningful data
ggplot(data = xt, aes(temp, oz.pred)) +
geom_point() +
geom_point(data = ozone, aes(temp, O3), col = 'red', shape = 'x')
# plot of predictions in black and actual values in red
xibh.s <- expand.grid(temp=0,ibh=seq(-3,3,0.1),ibt=0)
oz.s.pred <- predict(best.nn, newdata = x)
oz.pred <- oz.s.pred*sd(O3) + mean(O3)
xibh <- xibh*sd(ozone$ibh) + mean(ozone$ibh)
ggplot(data = xibh, aes(ibh, oz.pred)) +
geom_point() +
geom_point(data = ozone, aes(ibh, O3), col = 'red', shape = 'x')
xibh <- xibh*sd(ozone$ibh) + mean(ozone$ibh)
xt.s <- expand.grid(temp=seq(-3,3,0.1),ibh=0,ibt=0)
# standardised x values used
oz.s.pred <- predict(best.nn, newdata = x)
# the values predicted by the nn which used standardised data
oz.pred <- oz.s.pred*sd(O3) + mean(O3)
xt <- xt.s*sd(ozone$temp) + mean(ozone$temp)
# reverting standardised data to physically meaningful data
ggplot(data = xt, aes(temp, oz.pred)) +
geom_point() +
geom_point(data = ozone, aes(temp, O3), col = 'red', shape = 'x')
# plot of predictions in black and actual values in red
xibh.s <- expand.grid(temp=0,ibh=seq(-3,3,0.1),ibt=0)
oz.s.pred <- predict(best.nn, newdata = x)
oz.pred <- oz.s.pred*sd(O3) + mean(O3)
xibh <- xibh.s*sd(ozone$ibh) + mean(ozone$ibh)
ggplot(data = xibh, aes(ibh, oz.pred)) +
geom_point() +
geom_point(data = ozone, aes(ibh, O3), col = 'red', shape = 'x')
xt.s <- expand.grid(temp=seq(-3,3,0.1),ibh=0,ibt=0)
# standardised x values used
oz.s.pred.temp <- predict(best.nn, newdata = xt.s)
# the values predicted by the nn which used standardised data
oz.pred.temp <- oz.s.pred.temp*sd(O3) + mean(O3)
xt <- xt.s*sd(ozone$temp) + mean(ozone$temp)
# reverting standardised data to physically meaningful data
ggplot(data = xt, aes(temp, oz.pred.temp)) +
geom_point() +
geom_point(data = ozone, aes(temp, O3), col = 'red', shape = 'x')
# plot of predictions in black and actual values in red
xibh.s <- expand.grid(temp=0,ibh=seq(-3,3,0.1),ibt=0)
oz.s.pred.ibh <- predict(best.nn, newdata = xibh.s)
oz.pred.ibh <- oz.s.pred.ibh*sd(O3) + mean(O3)
xibh <- xibh.s*sd(ozone$ibh) + mean(ozone$ibh)
ggplot(data = xibh, aes(ibh, oz.pred.ibh)) +
geom_point() +
geom_point(data = ozone, aes(ibh, O3), col = 'red', shape = 'x')
xibt.s <- expand.grid(temp=0,ibh=0,ibt=seq(-3,3,0.1))
oz.s.pred.ibt <- predict(best.nn, newdata = xibt.s)
oz.pred <- oz.s.pred.ibt*sd(O3) + mean(O3)
xibh <- xibh.s*sd(ozone$ibh) + mean(ozone$ibh)
ggplot(data = xibh, aes(ibh, oz.pred.ibt)) +
geom_point() +
geom_point(data = ozone, aes(ibh, O3), col = 'red', shape = 'x')
xt.s <- expand.grid(temp=seq(-3,3,0.1),ibh=0,ibt=0)
# standardised x values used
oz.s.pred.temp <- predict(best.nn, newdata = xt.s)
# the values predicted by the nn which used standardised data
oz.pred.temp <- oz.s.pred.temp*sd(O3) + mean(O3)
xt <- xt.s*sd(ozone$temp) + mean(ozone$temp)
# reverting standardised data to physically meaningful data
ggplot(data = xt, aes(temp, oz.pred.temp)) +
geom_point() +
geom_point(data = ozone, aes(temp, O3), col = 'red', shape = 'x')
# plot of predictions in black and actual values in red
xibh.s <- expand.grid(temp=0,ibh=seq(-3,3,0.1),ibt=0)
oz.s.pred.ibh <- predict(best.nn, newdata = xibh.s)
oz.pred.ibh <- oz.s.pred.ibh*sd(O3) + mean(O3)
xibh <- xibh.s*sd(ozone$ibh) + mean(ozone$ibh)
ggplot(data = xibh, aes(ibh, oz.pred.ibh)) +
geom_point() +
geom_point(data = ozone, aes(ibh, O3), col = 'red', shape = 'x')
xibt.s <- expand.grid(temp=0,ibh=0,ibt=seq(-3,3,0.1))
oz.s.pred.ibt <- predict(best.nn, newdata = xibt.s)
oz.pred.ibt <- oz.s.pred.ibt*sd(O3) + mean(O3)
xibh <- xibh.s*sd(ozone$ibh) + mean(ozone$ibh)
ggplot(data = xibh, aes(ibh, oz.pred.ibt)) +
geom_point() +
geom_point(data = ozone, aes(ibh, O3), col = 'red', shape = 'x')
source("~/Library/CloudStorage/OneDrive-Personal/Documents/Programming/Rosalind/Rosalind Solutions.R", echo=TRUE)
source("~/Library/CloudStorage/OneDrive-Personal/Documents/Programming/Rosalind/Rosalind Solutions.R", echo=TRUE)
